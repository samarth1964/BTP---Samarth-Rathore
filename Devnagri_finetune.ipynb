{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets torch evaluate seqeval scikit-learn accelerate indic-transliteration tqdm sentencepiece\n"
      ],
      "metadata": {
        "id": "rt3hlaCzuCjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD6s-L1Urpfg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from huggingface_hub import login\n",
        "# import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress potential warnings from the model loading process\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ----------------------------\n",
        "# Install transliteration lib if missing (runs silently in Colab/Jupyter)\n",
        "# ----------------------------\n",
        "try:\n",
        "    from indic_transliteration import sanscript\n",
        "    from indic_transliteration.sanscript import transliterate\n",
        "except Exception:\n",
        "    try:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"indic-transliteration\", \"-q\"])\n",
        "        from indic_transliteration import sanscript\n",
        "        from indic_transliteration.sanscript import transliterate\n",
        "    except Exception as e:\n",
        "        print(\"Warning: Could not install 'indic-transliteration'. Transliteration might not work.\")\n",
        "        sanscript = None\n",
        "        transliterate = None\n",
        "\n",
        "# ============================================================\n",
        "# PATH AND DATA CONFIGURATION\n",
        "# ============================================================\n",
        "# data_dir = Path(\"cross_lingual_data\")  # <-- CHANGE THIS TO YOUR DATA FOLDER\n",
        "data_dir = Path('/content/drive/MyDrive/cross_lingual_data')\n",
        "languages = ['as', 'bn', 'gu', 'ml', 'mr', 'ta', 'te']  # languages to load\n",
        "model_name = \"ai4bharat/IndicNER\"\n",
        "output_dir = \"./indicbert-devanagari-ner-final\"\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"Data directory:\", data_dir.absolute())\n",
        "if not data_dir.exists():\n",
        "    print(\"FATAL ERROR: data directory not found. Exiting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ----------------------------\n",
        "# SCRIPT MAP + Transliteration Helper\n",
        "# ----------------------------\n",
        "SCRIPT_MAP = {}\n",
        "if sanscript is not None:\n",
        "    SCRIPT_MAP = {\n",
        "        'as': sanscript.BENGALI,\n",
        "        'bn': sanscript.BENGALI,\n",
        "        'gu': sanscript.GUJARATI,\n",
        "        'ml': sanscript.MALAYALAM,\n",
        "        'mr': sanscript.DEVANAGARI,\n",
        "        'ta': sanscript.TAMIL,\n",
        "        'te': sanscript.TELUGU,\n",
        "    }\n",
        "\n",
        "def transliterate_to_devanagari(text, lang_code):\n",
        "    \"\"\"Transliterate text (a token) from source script to Devanagari.\"\"\"\n",
        "    if text is None:\n",
        "        return text\n",
        "    if lang_code == 'mr':\n",
        "        return text\n",
        "    if transliterate is None or lang_code not in SCRIPT_MAP:\n",
        "        return text\n",
        "    try:\n",
        "        # Use ITRANS mode to handle non-Indic foreign words better\n",
        "        return transliterate(text, SCRIPT_MAP[lang_code], sanscript.DEVANAGARI)\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "# ============================================================\n",
        "# 1. AUTHENTICATION & MODEL LOADING (Your requested block)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(\"STEP 1: Hugging Face Authentication\")\n",
        "print(\"---\")\n",
        "try:\n",
        "    # Use existing token or prompt for login\n",
        "    login(new_session=False)\n",
        "    print(\"✓ Authentication check passed (token found or successfully logged in).\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Warning: Authentication failed. Error: {e}\")\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(f\"STEP 2: Loading Base Model '{model_name}'\")\n",
        "print(\"---\")\n",
        "\n",
        "try:\n",
        "    # Load the Tokenizer first\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    print(\"✓ Tokenizer loaded.\")\n",
        "\n",
        "    # Model will be loaded again later as AutoModelForTokenClassification\n",
        "    # We do a basic load here to verify access\n",
        "    _ = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "    print(\"✓ Base Model access verified.\")\n",
        "    del _\n",
        "    gc.collect()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n=============================================================\")\n",
        "    print(f\"❌ FATAL ERROR: Could not load model '{model_name}'\")\n",
        "    print(\"=============================================================\")\n",
        "    print(f\"Details: {e}\")\n",
        "    print(\"\\nACTION REQUIRED: Ensure you accepted the license terms on the model page.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ============================================================\n",
        "# 3. LOAD, CONVERT, AND SPLIT DATA\n",
        "# ============================================================\n",
        "\n",
        "def load_and_convert_data(file_path: Path, lang_code: str):\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        total = sum(1 for line in f if line.strip())\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f, total=total, desc=f\"Loading {file_path.name}\", ncols=80):\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                words = item.get(\"words\") or item.get(\"tokens\") or []\n",
        "                ner = item.get(\"ner\") or item.get(\"ner_tags\") or []\n",
        "                if len(words) != len(ner): continue\n",
        "                tokens_dev = [transliterate_to_devanagari(w, lang_code) for w in words]\n",
        "                data.append({\"tokens\": tokens_dev, \"ner_tags\": ner})\n",
        "            except Exception: continue\n",
        "    return data\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Loading and Converting Data to Devanagari\")\n",
        "print(\"=\"*70)\n",
        "all_data = []\n",
        "for lang in languages:\n",
        "    file_path = data_dir / f\"{lang}_data.json\"\n",
        "    if file_path.exists():\n",
        "        lang_examples = load_and_convert_data(file_path, lang)\n",
        "        print(f\"  Loaded {len(lang_examples):,} examples for {lang}\")\n",
        "        all_data.extend(lang_examples)\n",
        "        del lang_examples; gc.collect()\n",
        "    else:\n",
        "        print(\"  NOT FOUND:\", file_path)\n",
        "\n",
        "if len(all_data) == 0: sys.exit(1)\n",
        "\n",
        "# Split data\n",
        "train_val, test = train_test_split(all_data, test_size=0.10, random_state=42)\n",
        "train, val = train_test_split(train_val, test_size=0.10, random_state=42)\n",
        "del all_data; gc.collect()\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train),\n",
        "    \"validation\": Dataset.from_list(val),\n",
        "    \"test\": Dataset.from_list(test)\n",
        "})\n",
        "\n",
        "# Labels\n",
        "all_labels = set()\n",
        "for item in train: all_labels.update(item[\"ner_tags\"])\n",
        "label_list = sorted(list(all_labels))\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "print(f\"\\nLabels: {label_list}\")\n",
        "print(f\"Total examples: {len(train):,} (Train), {len(val):,} (Val), {len(test):,} (Test)\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. TOKENIZE AND ALIGN LABELS\n",
        "# ============================================================\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], is_split_into_words=True, truncation=True, padding=False, max_length=512\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label_seq[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100) # Subsequent sub-word token gets -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Tokenizing Dataset\")\n",
        "print(\"=\"*70)\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "# Load the specific model head for token classification\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 5. METRICS AND TRAINING\n",
        "# ============================================================\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "    for pred_seq, label_seq in zip(preds, labels):\n",
        "        pred_labels = []\n",
        "        true_label_list = []\n",
        "        for p, l in zip(pred_seq, label_seq):\n",
        "            if l != -100:\n",
        "                pred_labels.append(id2label[p])\n",
        "                true_label_list.append(id2label[l])\n",
        "        true_predictions.append(pred_labels)\n",
        "        true_labels.append(true_label_list)\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"]}\n",
        "\n",
        "Path(checkpoint_dir).mkdir(exist_ok=True)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint_dir, overwrite_output_dir=True, save_strategy=\"steps\", save_steps=1000,\n",
        "    save_total_limit=3, eval_strategy=\"steps\", eval_steps=1000, load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\", greater_is_better=True, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16, per_device_eval_batch_size=32, num_train_epochs=5,\n",
        "    weight_decay=0.01, gradient_accumulation_steps=2, fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=4, logging_dir=\"./logs\", logging_steps=100, logging_strategy=\"steps\",\n",
        "    seed=42, push_to_hub=False, report_to=\"none\"\n",
        ")\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"], tokenizer=tokenizer,\n",
        "    data_collator=data_collator, compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Training (with auto-resume if checkpoints exist)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Training IndicBERT\")\n",
        "print(\"=\"*70)\n",
        "existing_checkpoints = list(Path(checkpoint_dir).glob(\"checkpoint-*\"))\n",
        "if existing_checkpoints:\n",
        "    print(f\"Found {len(existing_checkpoints)} checkpoint(s). Resuming training.\")\n",
        "\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint=True if existing_checkpoints else None)\n",
        "    print(\"\\nTraining completed.\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user. Checkpoints saved.\")\n",
        "except Exception as e:\n",
        "    print(\"\\nTraining failed with exception:\", e)\n",
        "    raise\n",
        "\n",
        "# ============================================================\n",
        "# 6. SAVE FINAL MODEL AND EVALUATE\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: Saving and Evaluating\")\n",
        "print(\"=\"*70)\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nFinal model saved to: {output_dir}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
        "print(\"\\nTEST RESULTS\")\n",
        "print(f\"Precision: {test_results.get('eval_precision', 0.0):.4f}\")\n",
        "print(f\"Recall:    {test_results.get('eval_recall', 0.0):.4f}\")\n",
        "print(f\"F1 Score:  {test_results.get('eval_f1', 0.0):.4f}\")\n",
        "\n",
        "print(\"\\nDONE.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f010757f",
        "outputId": "830283f2-a69c-4a75-db02-e184d29fa3a2"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13cef15e",
        "outputId": "0efe3c2b-2463-4c74-cf93-4849d5bcb4a1"
      },
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# source_path = '/content/drive/MyDrive/cross_lingual_data'\n",
        "# destination_path = 'cross_lingual_data'\n",
        "\n",
        "# if os.path.exists(source_path):\n",
        "#     # Remove existing local folder to avoid errors during copy\n",
        "#     if os.path.exists(destination_path):\n",
        "#         shutil.rmtree(destination_path)\n",
        "#         print(f\"Removed existing directory: {destination_path}\")\n",
        "\n",
        "#     shutil.copytree(source_path, destination_path)\n",
        "#     print(f\"Successfully copied '{source_path}' to '{destination_path}'\")\n",
        "# else:\n",
        "#     print(f\"Error: Source folder '{source_path}' not found in Google Drive. Please ensure the folder exists and is named correctly.\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Source folder '/content/drive/MyDrive/cross_lingual_data' not found in Google Drive. Please ensure the folder exists and is named correctly.\n"
          ]
        }
      ]
    }
  ]
}